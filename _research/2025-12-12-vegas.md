---
title: "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering"
date: 2025-12-12
categories: [Research, Vision-Language Models]
tags: [Hallucination Mitigation, LVLM, Vision Encoder, Attention Steering, arXiv]
excerpt: "We propose VEGAS, a training-free inference-time method that uses the vision encoder's attention maps to guide the language model, significantly reducing hallucinations by steering focus towards key image objects."
---

### Abstract

Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map.

We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations.

Building on these insights, we introduce **VEGAS**, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

### Key Contributions
*   **Insight:** Identified that LVLM hallucinations are linked to diffuse visual attention maps in the language model, while the vision encoder maintains focused attention on key objects.
*   **Analysis:** Discovered that vision-text conflicts peak in the middle layers of the language model, making them the optimal intervention point.
*   **Method (VEGAS):** A training-free, inference-time intervention that injects the vision encoder's focused attention into the language model's middle layers to adaptively steer generation.
*   **Results:** Achieved state-of-the-art performance in hallucination reduction across multiple benchmarks.

### Publication Details
**Authors:** Zihu Wang\*, **Yuxuan Xia**\*, et al. (Assuming co-authorship based on context, please verify if needed)
**Venue:** arXiv:2512.12089 [cs.CV]
**Link:** [arXiv:2512.12089](https://arxiv.org/abs/2512.12089)
