---
title: "VEGAS: Mitigating Hallucinations in Large Vision-Language Models via Vision-Encoder Attention Guided Adaptive Steering"
date: 2025-12-12
categories: [Research, Vision-Language Models]
tags: [Hallucination Mitigation, LVLM, Vision Encoder, Attention Steering, arXiv]
excerpt: "We propose VEGAS, a training-free inference-time method that uses the vision encoder's attention maps to guide the language model, significantly reducing hallucinations by steering focus towards key image objects."
---

### Abstract

Large vision-language models (LVLMs) exhibit impressive ability to jointly reason over visual and textual inputs. However, they often produce outputs that are linguistically fluent but factually inconsistent with the visual evidence, i.e., they hallucinate. Despite growing efforts to mitigate such hallucinations, a key question remains: what form of visual attention can effectively suppress hallucinations during decoding? In this work, we provide a simple answer: the vision encoder's own attention map.

We show that LVLMs tend to hallucinate when their final visual-attention maps fail to concentrate on key image objects, whereas the vision encoder's more concentrated attention maps substantially reduce hallucinations. To further investigate the cause, we analyze vision-text conflicts during decoding and find that these conflicts peak in the language model's middle layers. Injecting the vision encoder's attention maps into these layers effectively suppresses hallucinations.

Building on these insights, we introduce **VEGAS**, a simple yet effective inference-time method that integrates the vision encoder's attention maps into the language model's mid-layers and adaptively steers tokens which fail to concentrate on key image objects. Extensive experiments across multiple benchmarks demonstrate that VEGAS consistently achieves state-of-the-art performance in reducing hallucinations.

### Method Overview

![VEGAS Framework](/assets/images/vegas_overview_L.pdf)

As illustrated in the overview, VEGAS operates by leveraging the **intrinsic attention maps of the Vision Encoder (e.g., CLIP)**, which naturally possess a high degree of object-centric focus. The framework consists of two key stages:

1.  **Attention Extraction:** We extract the self-attention maps from the Vision Encoder, which serve as a "ground truth" for where the model *should* be looking.
2.  **Adaptive Steering:** During the decoding process of the Large Language Model (LLM), we monitor the attention distribution. When the LLM's attention becomes diffuse or drifts away from relevant image regions (a precursor to hallucination), VEGAS intervenes. It injects the focused attention from the Vision Encoder into the **middle layers** of the LLM, effectively "steering" the generation back to the correct visual evidence.

This approach is entirely **training-free** and operates solely at inference time, making it a plug-and-play solution for existing LVLMs.

### Key Contributions
*   **Insight:** Identified that LVLM hallucinations are linked to diffuse visual attention maps in the language model, while the vision encoder maintains focused attention on key objects.
*   **Analysis:** Discovered that vision-text conflicts peak in the middle layers of the language model, making them the optimal intervention point.
*   **Method (VEGAS):** A training-free, inference-time intervention that injects the vision encoder's focused attention into the language model's middle layers to adaptively steer generation.
*   **Results:** Achieved state-of-the-art performance in hallucination reduction across multiple benchmarks.

### Publication Details
**Authors:** Zihu Wang\*, **Yuxuan Xia**\*, et al.
**Venue:** arXiv:2512.12089 [cs.CV]
**Link:** [arXiv:2512.12089](https://arxiv.org/abs/2512.12089)
