---
title: "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models"
date: 2026-01-07
categories: [Research, Vision-Language Models]
tags: [Hallucination Mitigation, Contrastive Decoding, LVLM, Visual Bias, arXiv]
excerpt: "Introducing SDCD, a training-free algorithm that mitigates hallucinations by calibrating model predictions against a structure-disrupted view, effectively suppressing texture-driven biases."
---

### Abstract

Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process.

We identify that **visual statistical bias**, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations.

To address this, we introduce a training-free algorithm called **Structure-Disrupted Contrastive Decoding (SDCD)**, which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

### The "Bag-of-Patches" Phenomenon

Why do LVLMs hallucinate? A key reason lies in how their vision encoders (like CLIP or ViT) process images.

![CLIP Bag-of-Patches Analysis](/assets/images/sdcd_CLIP_BoP.png)
*Figure 1: Analysis of CLIP's attention behavior.*

![ViT Bag-of-Patches Analysis](/assets/images/sdcd_ViT_BoP.png)
*Figure 2: Analysis of ViT's attention behavior.*

As illustrated in Figures 1 and 2, we observe that Vision Encoders often behave like a **"Bag-of-Patches"**. They are highly sensitive to local textures (e.g., fur, patterns) but surprisingly insensitive to the global arrangement of these patches.

Surprisingly, **even when the input image is completely shuffled**, destroying all global structural information, the performance metrics remain largely unchanged. Specifically, the classification accuracy of **ViT** and the retrieval recall of **CLIP** on shuffled images are comparable to those on original images.

This finding strongly suggests that these models heavily rely on **local-patch texture information** rather than holistic geometric understanding. This over-reliance on texture is a major source of hallucinations, as the model may confidently detect an object solely based on texture cues, even when the object's structure is absent or distorted.

### Structure Sensitivity Divergence in LVLMs

We further investigate how this "Bag-of-Patches" behavior impacts the decoding process of LVLMs. By monitoring the logit dynamics of "Yes" (object present) and "No" (object absent) tokens under both the original view ($V$) and the structure-disrupted view ($V^\prime$), we observe a significant divergence in how the model responds to real versus hallucinated objects.

<div style="display: flex; gap: 20px; margin-bottom: 20px;">
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_llava_gt_yes.jpg" alt="Structure Sensitivity for Real Objects" style="width: 100%;">
    <p><em>(a) GT = Yes (Real Objects)</em></p>
  </div>
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_llava_gt_no.jpg" alt="Structure Sensitivity for Hallucinations" style="width: 100%;">
    <p><em>(b) GT = No (Hallucinations)</em></p>
  </div>
</div>

*   **For Real Objects (a):** When we disrupt the image structure ($V^\prime$), the model's confidence in the correct answer ("Yes") drops sharply. This indicates that recognizing real objects relies heavily on **structural consistency** across patches.
*   **For Hallucinated Objects (b):** Conversely, structural disruption often leads to an **increase** in confidence for the incorrect "Yes" token. This "Texture Unleashed" phenomenon suggests that hallucinations are primarily driven by local texture cues, which become even more dominant when global structure is removed.

This asymmetric sensitivity—where real objects are penalized by structural loss while hallucinations are enhanced—provides the core motivation for SDCD. We can use the structure-disrupted view as a negative control to suppress texture-induced hallucinations.

### Structure-Disrupted Contrastive Decoding (SDCD)

Building upon the analysis of Structure Sensitivity Divergence, I identify that the shuffled view $V^\prime$ undermines global structural constraints. This degradation induces the model to rely on local textures for generation, which is a behavior intrinsically linked to object hallucination. To address this, I propose Structure-Disrupted Contrastive Decoding (SDCD).

SDCD leverages shuffled view $V^\prime$ to eliminate texture biases stemming from the Bag-of-Patches behavior within the generated logits. Specifically, given a textual query $x$ and the original view $V$, the model yields two independent output distributions: $p_{\theta}(y|V, x)$ conditioned on the original view, and $p_{\theta}(y|V^\prime, x)$ conditioned on the structure-disrupted (i.e., patch-shuffled) view. Since $V^\prime$ disrupts global geometric constraints, the resulting distribution $p_{\theta}(y|V^\prime, x)$ essentially characterizes the model's hallucination-prone distribution derived solely from local textural cues.

To suppress this bias, SDCD performs contrastive calibration on the two decoding results within the logit space. The final decoding distribution is defined as:

$$
\begin{aligned} 
p_{\text{SDCD}}(y_t \mid V, V^\prime, x) 
&= \operatorname{softmax}\big( 
(1+\alpha)\,\operatorname{logit}_{\theta}(y_t \mid V, x) \\ 
&\qquad {}- \alpha\,\operatorname{logit}_{\theta}(y_t \mid V^\prime, x) 
\big), 
\end{aligned}
$$

where $\alpha \geq 0$ is a contrastive hyperparameter controlling the penalty strength on texture bias. SDCD serves as a Structure-Aware Corrective Mechanism:

*   For ground-truth objects, the generation logits of their corresponding tokens are highly sensitive to structural disruption and drop significantly under $V^\prime$. In this case, contrastive calibration primarily preserves and reinforces the structural consistency evidence from the original view.
*   For potential hallucinated objects, the corresponding tokens may maintain high logits or even exhibit an increase due to texture induction after structural disruption. In such scenarios, the contrastive term directly suppresses their probability in the final decoding distribution.

By penalizing tokens that retain high confidence under structure-less conditions, SDCD effectively mitigates the interference of texture-dominated biases in the generation process.

### Publication Details
**Authors:** **Yuxuan Xia**, et al.
**Venue:** arXiv:2601.03500 [cs.CV]
**Link:** [arXiv:2601.03500](https://arxiv.org/abs/2601.03500v1)
