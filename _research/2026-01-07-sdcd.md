---
title: "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models"
date: 2026-01-07
categories: [Research, Vision-Language Models]
tags: [Hallucination Mitigation, Contrastive Decoding, LVLM, Visual Bias, arXiv]
excerpt: "Introducing SDCD, a training-free algorithm that mitigates hallucinations by calibrating model predictions against a structure-disrupted view, effectively suppressing texture-driven biases."
---

### Abstract

Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process.

We identify that **visual statistical bias**, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations.

To address this, we introduce a training-free algorithm called **Structure-Disrupted Contrastive Decoding (SDCD)**, which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

### Method Overview

![SDCD Framework](/assets/images/sdcd_overview.jpg)

The SDCD framework tackles hallucinations by directly addressing the **texture-bias** inherent in Vision Encoders. As shown in the figure, the process involves a contrastive comparison between two visual views:

1.  **Original View:** The model processes the standard, intact image, producing a probability distribution over the next token.
2.  **Structure-Disrupted View:** We generate a "negative" sample by randomly **shuffling the image patches**. This destroys the global geometric structure while preserving local texture statistics.
3.  **Contrastive Decoding:** By subtracting the logits of the structure-disrupted view from the original view, SDCD penalizes tokens that rely solely on texture (which would be high in both views) and boosts tokens that depend on valid structural information (which is absent in the disrupted view).

This **contrastive calibration** effectively filters out hallucinations caused by spurious texture correlations, ensuring the model grounds its generation in the actual visual structure.

### Key Contributions
*   **Identification of Visual Bias:** Pinpointed "visual statistical bias" caused by the Bag-of-Patches nature of vision encoders as a key source of hallucinations.
*   **Method (SDCD):** Proposed a novel contrastive decoding strategy that uses a structure-disrupted (shuffled) image view as a negative constraint to penalize texture-only reliance.
*   **Training-Free:** The method requires no retraining or fine-tuning of the model.
*   **Performance:** Demonstrated significant reduction in object hallucinations and improved multimodal reasoning capabilities.

### Publication Details
**Authors:** **Yuxuan Xia**, et al.
**Venue:** arXiv:2601.03500 [cs.CV]
**Link:** [arXiv:2601.03500](https://arxiv.org/abs/2601.03500v1)
