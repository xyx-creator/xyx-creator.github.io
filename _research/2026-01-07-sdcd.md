---
title: "SDCD: Structure-Disrupted Contrastive Decoding for Mitigating Hallucinations in Large Vision-Language Models"
date: 2026-01-07
categories: [Research, Vision-Language Models]
tags: [Hallucination Mitigation, Contrastive Decoding, LVLM, Visual Bias, arXiv]
excerpt: "Introducing SDCD, a training-free algorithm that mitigates hallucinations by calibrating model predictions against a structure-disrupted view, effectively suppressing texture-driven biases."
---

### Abstract

Large Vision-Language Models (LVLMs) demonstrate significant progress in multimodal understanding and reasoning, yet object hallucination remains a critical challenge. While existing research focuses on mitigating language priors or high-level statistical biases, they often overlook the internal complexities of the visual encoding process.

We identify that **visual statistical bias**, arising from the inherent Bag-of-Patches behavior of Vision Encoders under weak structural supervision, acts as a contributing factor of object hallucinations. Under this bias, models prioritize local texture features within individual patches over holistic geometric structures. This tendency may induce spurious visual confidence and result in hallucinations.

To address this, we introduce a training-free algorithm called **Structure-Disrupted Contrastive Decoding (SDCD)**, which performs contrastive calibration of the output distribution by introducing a shuffled structure-disrupted view. By penalizing tokens that maintain high confidence under this structure-less view, SDCD effectively suppresses the texture-driven bias. Experimental results demonstrate that SDCD significantly mitigates hallucinations across multiple benchmarks and enhances the overall multimodal capabilities of LVLMs.

### The "Bag-of-Patches" Phenomenon

Why do LVLMs hallucinate? A key reason lies in how their vision encoders (like CLIP or ViT) process images.

<div style="display: flex; gap: 20px; margin-bottom: 20px; align-items: flex-start;">
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_ViT_BoP.png" alt="ViT Bag-of-Patches Analysis" style="height: 300px; width: auto; max-width: 100%; object-fit: contain;">
    <p><em>(a) ViT</em></p>
  </div>
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_CLIP_BoP.png" alt="CLIP Bag-of-Patches Analysis" style="height: 300px; width: auto; max-width: 100%; object-fit: contain;">
    <p><em>(b) CLIP</em></p>
  </div>
</div>
*Figure 1: Quantitative analysis of the Bag-of-Patches behavior.*

As illustrated in Figure 1, we observe that Vision Encoders often behave like a **"Bag-of-Patches"**. They are highly sensitive to local textures (e.g., fur, patterns) but surprisingly insensitive to the global arrangement of these patches.

Surprisingly, **even when the input image is completely shuffled**, destroying all global structural information, the performance metrics remain largely unchanged. Specifically, the classification accuracy of **ViT** and the retrieval recall of **CLIP** on shuffled images are comparable to those on original images.

This finding strongly suggests that these models heavily rely on **local-patch texture information** rather than holistic geometric understanding. This over-reliance on texture is a major source of hallucinations, as the model may confidently detect an object solely based on texture cues, even when the object's structure is absent or distorted.

### Structure Sensitivity Divergence in LVLMs

We further investigate how this "Bag-of-Patches" behavior impacts the decoding process of LVLMs. By monitoring the logit dynamics of "Yes" (object present) and "No" (object absent) tokens under both the original view (V) and the structure-disrupted view (V‘), we observe a **Structure Sensitivity Divergence** in how the model responds to real versus hallucinated objects.

<div style="display: flex; gap: 20px; margin-bottom: 20px;">
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_llava_gt_yes.jpg" alt="Structure Sensitivity for Real Objects" style="width: 100%;">
    <p><em>(a) GT = Yes (Real Objects)</em></p>
  </div>
  <div style="flex: 1; text-align: center;">
    <img src="/assets/images/sdcd_llava_gt_no.jpg" alt="Structure Sensitivity for Hallucinations" style="width: 100%;">
    <p><em>(b) GT = No (Hallucinations)</em></p>
  </div>
</div>

*   **For Real Objects (a):** When we disrupt the image structure (V‘), the model's confidence in the correct answer ("Yes") **drops sharply**. This indicates that recognizing real objects relies heavily on **structural consistency** across patches.
*   **For Hallucinated Objects (b):** Conversely, structural disruption often leads to an **increase** in confidence for the incorrect "Yes" token. This "Texture Unleashed" phenomenon suggests that hallucinations are primarily driven by local texture cues, which become even more dominant when global structure is removed.

This asymmetric sensitivity—where real objects are penalized by structural loss while hallucinations are enhanced—provides the core motivation for SDCD. We can use the structure-disrupted view as a negative control to suppress texture-induced hallucinations.

### Structure-Disrupted Contrastive Decoding (SDCD)

Building upon the analysis of Structure Sensitivity Divergence, I identify that the shuffled view V‘ undermines global structural constraints. This degradation induces the model to rely on local textures for generation, which is a behavior intrinsically linked to object hallucination. To address this, I propose Structure-Disrupted Contrastive Decoding (SDCD).

To suppress this bias, SDCD performs contrastive calibration on the two decoding results within the logit space. The final decoding distribution is defined as:

$$
\begin{aligned} 
p_{\text{SDCD}}(y_t \mid V, V‘, x) 
&= \operatorname{softmax}\big( 
(1+α)\,\operatorname{logit}_{\theta}(y_t \mid V, x) \\ 
&\qquad {}- α\,\operatorname{logit}_{\theta}(y_t \mid V‘, x) 
\big), 
\end{aligned}
$$

where α ≥ 0 is a contrastive hyperparameter controlling the penalty strength on texture bias. SDCD serves as a Structure-Aware Corrective Mechanism:

*   For ground-truth objects, the generation logits of their corresponding tokens are highly sensitive to structural disruption and drop significantly under V‘. In this case, contrastive calibration primarily preserves and reinforces the structural consistency evidence from the original view.
*   For potential hallucinated objects, the corresponding tokens may maintain high logits or even exhibit an increase due to texture induction after structural disruption. In such scenarios, the contrastive term directly suppresses their probability in the final decoding distribution.

By penalizing tokens that retain high confidence under structure-less conditions, SDCD effectively mitigates the interference of texture-dominated biases in the generation process.

### Experiment Results

We evaluated SDCD on the POPE benchmark using representative models with MLP projectors (LLaVA-1.5 and Qwen2.5-VL). The results demonstrate that SDCD consistently outperforms both standard decoding (Regular) and the visual contrastive decoding baseline (VCD).

<div class="table-wrapper">
<table>
  <thead>
    <tr>
      <th rowspan="2">Dataset</th>
      <th rowspan="2">Model</th>
      <th rowspan="2" class="group-divider">Decoding</th>
      <th colspan="4" class="group-divider">Random</th>
      <th colspan="4" class="group-divider">Popular</th>
      <th colspan="4">Adversarial</th>
    </tr>
    <tr>
      <th>Acc.</th>
      <th>Prec.</th>
      <th>Rec.</th>
      <th class="group-divider">F1</th>
      <th>Acc.</th>
      <th>Prec.</th>
      <th>Rec.</th>
      <th class="group-divider">F1</th>
      <th>Acc.</th>
      <th>Prec.</th>
      <th>Rec.</th>
      <th>F1</th>
    </tr>
  </thead>
  <tbody>
    <!-- MSCOCO -->
    <tr>
      <td rowspan="6">MSCOCO</td>
      <td rowspan="3">LLaVA1.5</td>
      <td class="group-divider">Regular</td>
      <td>82.93</td><td>92.01</td><td>72.13</td><td class="group-divider">80.87</td>
      <td>81.10</td><td>87.90</td><td>72.13</td><td class="group-divider">79.24</td>
      <td>78.63</td><td>82.96</td><td>72.07</td><td>77.13</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>84.87</td><td>92.52</td><td>75.87</td><td class="group-divider">83.37</td>
      <td>82.43</td><td>87.34</td><td>75.87</td><td class="group-divider">81.20</td>
      <td>79.90</td><td>82.52</td><td>75.87</td><td>79.06</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border);">
      <td class="group-divider">SDCD</td>
      <td class="bold">85.90</td><td class="bold">93.46</td><td class="bold">77.20</td><td class="bold group-divider">84.56</td>
      <td class="bold">83.43</td><td class="bold">88.19</td><td class="bold">77.20</td><td class="bold group-divider">82.33</td>
      <td class="bold">80.87</td><td class="bold">83.36</td><td class="bold">77.13</td><td class="bold">80.12</td>
    </tr>
    <tr>
      <td rowspan="3">Qwen2.5-VL</td>
      <td class="group-divider">Regular</td>
      <td>81.73</td><td>98.37</td><td>64.53</td><td class="group-divider">77.94</td>
      <td>81.23</td><td class="bold">97.66</td><td>64.00</td><td class="group-divider">77.33</td>
      <td>80.83</td><td>95.84</td><td>64.47</td><td>77.08</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>82.67</td><td>98.80</td><td>66.13</td><td class="group-divider">79.23</td>
      <td>82.97</td><td>97.50</td><td>67.67</td><td class="group-divider">79.89</td>
      <td>81.77</td><td class="bold">96.40</td><td>66.00</td><td>78.35</td>
    </tr>
    <tr style="border-bottom: 2px solid var(--text-primary);">
      <td class="group-divider">SDCD</td>
      <td class="bold">85.63</td><td class="bold">98.90</td><td class="bold">72.07</td><td class="bold group-divider">83.38</td>
      <td class="bold">85.07</td><td>97.05</td><td class="bold">72.33</td><td class="bold group-divider">82.89</td>
      <td class="bold">83.93</td><td>94.57</td><td class="bold">72.00</td><td class="bold">81.76</td>
    </tr>
    
    <!-- A-OKVQA -->
    <tr>
      <td rowspan="6">A-OKVQA</td>
      <td rowspan="3">LLaVA1.5</td>
      <td class="group-divider">Regular</td>
      <td>84.03</td><td>87.68</td><td>79.20</td><td class="group-divider">83.22</td>
      <td>80.23</td><td class="bold">80.87</td><td>79.20</td><td class="group-divider">80.03</td>
      <td>74.27</td><td class="bold">72.33</td><td>78.60</td><td>75.34</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>85.03</td><td>87.09</td><td>82.27</td><td class="group-divider">84.61</td>
      <td>80.60</td><td>79.61</td><td>82.27</td><td class="group-divider">80.92</td>
      <td class="bold">74.90</td><td>71.75</td><td>82.13</td><td class="bold">76.59</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border);">
      <td class="group-divider">SDCD</td>
      <td class="bold">86.30</td><td class="bold">87.79</td><td class="bold">84.33</td><td class="bold group-divider">86.03</td>
      <td class="bold">80.93</td><td>78.96</td><td class="bold">84.33</td><td class="bold group-divider">81.56</td>
      <td>73.93</td><td>70.19</td><td class="bold">83.20</td><td>76.14</td>
    </tr>
    <tr>
      <td rowspan="3">Qwen2.5-VL</td>
      <td class="group-divider">Regular</td>
      <td>84.43</td><td>94.41</td><td>73.20</td><td class="group-divider">82.46</td>
      <td>83.23</td><td class="bold">91.51</td><td>73.27</td><td class="group-divider">81.38</td>
      <td>77.43</td><td>80.64</td><td>72.20</td><td>76.19</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>86.43</td><td class="bold">95.89</td><td>76.13</td><td class="group-divider">84.88</td>
      <td>84.23</td><td>91.31</td><td>75.67</td><td class="group-divider">82.76</td>
      <td>80.13</td><td class="bold">82.80</td><td>76.07</td><td>79.29</td>
    </tr>
    <tr style="border-bottom: 2px solid var(--text-primary);">
      <td class="group-divider">SDCD</td>
      <td class="bold">88.40</td><td>95.57</td><td class="bold">80.53</td><td class="bold group-divider">87.41</td>
      <td class="bold">86.07</td><td>90.62</td><td class="bold">80.47</td><td class="bold group-divider">85.24</td>
      <td class="bold">80.80</td><td>81.30</td><td class="bold">80.00</td><td class="bold">80.65</td>
    </tr>

    <!-- GQA -->
    <tr>
      <td rowspan="6">GQA</td>
      <td rowspan="3">LLaVA1.5</td>
      <td class="group-divider">Regular</td>
      <td>83.60</td><td>87.11</td><td>78.87</td><td class="group-divider">82.79</td>
      <td>77.87</td><td class="bold">77.32</td><td>78.87</td><td class="group-divider">78.09</td>
      <td>75.17</td><td class="bold">73.32</td><td>79.13</td><td>76.11</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>84.83</td><td>86.56</td><td>82.47</td><td class="group-divider">84.47</td>
      <td class="bold">78.07</td><td>75.80</td><td>82.47</td><td class="bold group-divider">78.99</td>
      <td class="bold">75.50</td><td>72.49</td><td>82.20</td><td class="bold">77.04</td>
    </tr>
    <tr style="border-bottom: 1px solid var(--border);">
      <td class="group-divider">SDCD</td>
      <td class="bold">85.70</td><td class="bold">87.16</td><td class="bold">83.73</td><td class="bold group-divider">85.41</td>
      <td>76.47</td><td>73.11</td><td class="bold">83.73</td><td>78.06</td>
      <td>74.50</td><td>70.50</td><td class="bold">84.27</td><td>76.77</td>
    </tr>
    <tr>
      <td rowspan="3">Qwen2.5-VL</td>
      <td class="group-divider">Regular</td>
      <td>84.40</td><td>93.80</td><td>73.67</td><td class="group-divider">82.52</td>
      <td>80.63</td><td>85.81</td><td>73.40</td><td class="group-divider">79.12</td>
      <td>78.53</td><td>81.29</td><td>74.13</td><td>77.55</td>
    </tr>
    <tr>
      <td class="group-divider">VCD</td>
      <td>86.50</td><td class="bold">95.97</td><td>76.20</td><td class="group-divider">84.95</td>
      <td>82.57</td><td class="bold">87.55</td><td>75.93</td><td class="group-divider">81.33</td>
      <td>80.43</td><td>83.54</td><td>75.80</td><td>79.48</td>
    </tr>
    <tr>
      <td class="group-divider">SDCD</td>
      <td class="bold">88.30</td><td>95.70</td><td class="bold">80.20</td><td class="bold group-divider">87.27</td>
      <td class="bold">83.87</td><td>85.88</td><td class="bold">81.07</td><td class="bold group-divider">83.40</td>
      <td class="bold">82.70</td><td class="bold">84.04</td><td class="bold">80.73</td><td class="bold">82.35</td>
    </tr>
  </tbody>
</table>
</div>
*Table 1: Results for LLaVA1.5 and Qwen2.5-VL on POPE (values in %). Best results are highlighted in bold.*

As shown in Table 1, our proposed SDCD outperforms both standard decoding (Regular) and the visual contrastive decoding baseline (VCD) across the majority of settings.

*   **For LLaVA-1.5:** SDCD achieves the best Accuracy and F1 scores across all three evaluation dimensions on MSCOCO. Specifically, under the Random setting, SDCD reaches an accuracy of **85.90%**, surpassing Regular (82.93%) and VCD (84.87%). On A-OKVQA and GQA, SDCD maintains a similar advantage, particularly in Recall metrics.
*   **For Qwen2.5-VL:** Comparable improvements are observed. This model exhibits a distinct "high precision, low recall" characteristic in Regular mode (e.g., 98.37% Precision versus 64.53% Recall in MSCOCO Random), reflecting an extreme conservative tendency. SDCD increases Recall by approximately **7.5 percentage points** to 72.07% while maintaining high precision, thereby achieving the highest F1 score (83.38%). This indicates that SDCD effectively corrects the model's bias towards over-refusal, mitigating missed detections without introducing additional hallucinations.

### Publication Details
**Authors:** **Yuxuan Xia**, et al.
**Venue:** arXiv:2601.03500 [cs.CV]
**Link:** [arXiv:2601.03500](https://arxiv.org/abs/2601.03500v1)
